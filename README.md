# Mapping the Sequence-Structure-Function Space of Amyloids: A Data-Centric Study of Aggregation Criteria

This repository contains the complete data processing pipeline for creating a unified, evidence-weighted dataset for amyloidogenicity research. The project addresses the critical challenge of data heterogeneity and lack of standardization in the field of amyloid studies by integrating multiple public databases into a single, coherent resource with a quantitative confidence score for each entry.

## Project Overview

The study of amyloid proteins is hampered by the inconsistent and often conflicting data scattered across numerous databases. The definition of "amyloidogenicity" itself varies, depending on the experimental methods used for characterization. This project implements a systematic framework to:

1.  **Integrate Data**: Consolidate data from major amyloid databases, including WALTZ-DB 2.0, Cross-Beta DB, AmyLoad, and CPAD 2.0.
2.  **Standardize Evidence**: Map dozens of raw experimental method names from source files into a universal, tiered vocabulary using a rule-based engine (`Methods_Mapping/build_universal_methods_mapping.py`).
3.  **Quantify Confidence**: Assign a quantitative, evidence-based confidence score to each protein or peptide based on the strength of the experimental evidence supporting its amyloidogenic classification.
4.  **Resolve Conflicts**: Use a weighted-consensus algorithm to resolve conflicting classifications for the same sequence, prioritizing higher-quality evidence (e.g., structural data over staining assays).
5.  **Generate Unified Datasets**: Produce clean, deduplicated, and ready-to-use datasets for computational modeling and analysis.

## Repository Structure

```
/
├── Databases/
│   ├── amyloid_pipeline.py         # Main data unification and processing pipeline
│   ├── AmyLoad_session_unlogged_unlogged_.csv
│   ├── crossbetadb.json
│   ├── waltzdb.csv
│   ├── aggregatingpeptides.xlsx
│   └── amyloidstructure.xlsx
│   └── README.md                   # Detailed information about the data and pipeline
│
├── Methods_Mapping/
│   ├── build_universal_methods_mapping.py  # Script to create the method mapping table
│   └── README.md                   # Detailed information about the methods mapping script
│
└── README.md                       # This file (main project overview)
```

## Core Components

### 1. Methods Mapping (`Methods_Mapping/`)

Before processing the data, we first need to understand the variety of experimental methods reported in the source files. The `build_universal_methods_mapping.py` script scans all source databases and extracts the unique names of experimental techniques. It then applies a set of regex-based rules to map these raw names to a standardized vocabulary. Each standardized method is assigned to a tier and given a confidence level, which is crucial for the evidence-weighting step in the main pipeline.

### 2. Data Unification Pipeline (`Databases/`)

The `amyloid_pipeline.py` script is the core of this project. It takes the raw database files as input and performs the following steps:

-   **Parsing**: Reads and parses various file formats (`.csv`, `.json`, `.xlsx`).
-   **Standardization**: Converts each record into a standardized `AmyloidSequenceEntry` or `AmyloidStructureEntry` format.
-   **Evidence Weighting**: Assigns a weight to each entry based on the experimental evidence, using the mapping generated by the methods script.
-   **Deduplication**: Uses a biologically-aware deduplication key `(sequence, protein_id, region_start, region_end)` to identify and group unique entries.
-   **Conflict Resolution**: For entries with conflicting labels (i.e., reported as both 'amyloid' and 'non-amyloid'), a weighted consensus algorithm decides the final label. If the evidence is not clearly decisive, the entry is moved to a separate 'conflicts' dataset.
-   **Output Generation**: Generates several clean datasets as `.csv` files, including a consensus dataset, a non-amyloid dataset, a dataset of unresolved conflicts, and a dataset of structural entries.

## Getting Started

### Prerequisites

-   Python 3.8+
-   Pandas
-   NumPy
-   Openpyxl

### Installation

Clone the repository:
```bash
git clone <repository-url>
cd <repository-name>
```

Install the required Python packages:
```bash
pip install pandas numpy openpyxl
```

### Usage

1.  **Generate the Methods Map**: First, run the methods mapping script to create the universal vocabulary. Make sure to update the file paths inside the script.

    ```bash
    python Methods_Mapping/build_universal_methods_mapping.py
    ```

2.  **Run the Main Pipeline**: Once the methods map is generated, run the main data unification pipeline. Update the file paths in the `run_pipeline` function call at the bottom of the script.

    ```bash
    python Databases/amyloid_pipeline.py
    ```

    The script will generate the final, cleaned datasets in the specified output directory.


